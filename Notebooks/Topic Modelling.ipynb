{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "#import pyLDAvis.gensim\n",
    "import warnings\n",
    "import csv\n",
    "from datetime import datetime as dt\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*use @default decorator instead.*')\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))       # Changing the cell widths\n",
    "\n",
    "pd.options.display.max_rows = 30                                            # Setting the max number of rows\n",
    "pd.options.display.max_columns = 50                                         # Setting the max number of columns\n",
    "\n",
    "#pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'data/'    # Data Directory \n",
    "out = 'Outputs/'  # Output Directory\n",
    "msg = 'messages.csv'                       # Input Dataset\n",
    "\n",
    "sample_size = 5000\n",
    "topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_msg_in = pd.read_csv(path + msg)\n",
    "df_msg_en = df_msg_in[(df_msg_in['language'] == 'EN')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Message(object):\n",
    "    def __init__(self, thread_id, date_time, message_id, user_id, language, msg_type, msg_body):\n",
    "        self.thread_id = thread_id\n",
    "        self.date_time = date_time\n",
    "        self.message_id = message_id\n",
    "        self.user_id = user_id\n",
    "        self.language = language\n",
    "        self.msg_type = msg_type\n",
    "        self.msg_body = msg_body\n",
    "        \n",
    "    \n",
    "class User(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def loadData(f):\n",
    "    users = {}\n",
    "    messages = []\n",
    "    with open(f, \"r\") as data:\n",
    "        # \"thread_id\",\"date_time\",\"message_id\",\"user_id\",\"language\",\"type\",\"body\"\n",
    "        # datetime format: 2015-02-09 14:27:05\n",
    "        reader = csv.DictReader(data)\n",
    "        for row in reader:\n",
    "            user_id = row[\"user_id\"]\n",
    "            message = Message(row[\"thread_id\"],\n",
    "                              dt.strptime(row[\"date_time\"], \"%Y-%m-%d %H:%M:%S\"),\n",
    "                              row[\"message_id\"],\n",
    "                              row[\"user_id\"],\n",
    "                              row[\"language\"],\n",
    "                              row[\"type\"],\n",
    "                              row[\"body\"])\n",
    "            if user_id not in users:\n",
    "                users[user_id] = []\n",
    "            users[user_id].append(message)\n",
    "            messages.append(message)\n",
    "    return users, messages\n",
    "\n",
    "def getMessageGroups(messages, grouper):\n",
    "    groupedMessages = {}\n",
    "    for message in messages:\n",
    "        messageGroup = getattr(message, grouper)\n",
    "        if messageGroup not in groupedMessages:\n",
    "            groupedMessages[messageGroup] = []\n",
    "        groupedMessages[messageGroup].append(message)\n",
    "    return groupedMessages\n",
    "        \n",
    "\n",
    "def cleaner(row):\n",
    "    '''Function to clean the text data and prep for further analysis'''\n",
    "    stops = set(stopwords.words(\"english\"))     # Creating a set of Stopwords\n",
    "    p_stemmer = PorterStemmer()                 # Creating the stemmer model\n",
    "    text = row['body'].lower()                   # Converts to lower case\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)          # Removes punctuation\n",
    "    text = re.sub(\"cyclist\",\"cycl\",text)         # Manual intervention for 'cyclist'\n",
    "    text = text.split()                          # Splits the data into individual words \n",
    "    text = [w for w in text if not w in stops]   # Removes stopwords\n",
    "    text = [p_stemmer.stem(i) for i in text]     # Stemming (reducing words to their root)\n",
    "    return text\n",
    "\n",
    "\n",
    "def messages_vectorizer(messages):\n",
    "    '''Function to take a message object and convert it to a list of terms'''\n",
    "    stops = set(stopwords.words(\"english\"))     # Creating a set of Stopwords\n",
    "    p_stemmer = PorterStemmer()                 # Creating the stemmer model\n",
    "    text = ''\n",
    "    for m in messages:\n",
    "        text = text + ' ' + m.msg_body.lower()          # Converts to lower case\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    text = text.split()                          # Splits the data into individual words \n",
    "    text = [w for w in text if not w in stops]   # Removes stopwords\n",
    "    text = [p_stemmer.stem(i) for i in text]     # Stemming (reducing words to their root)\n",
    "    return text\n",
    "\n",
    "def model(data):\n",
    "    data_dict = corpora.Dictionary(data)                       # Creates an id <-> term dictionary\n",
    "    data_corpus = [data_dict.doc2bow(text) for text in data]     # convert tokenized documents into a document-term matrix\n",
    "    data_model = gensim.models.ldamodel.LdaModel(data_corpus, \n",
    "                                                   num_topics=topics, \n",
    "                                                   id2word = data_dict,\n",
    "                                                   passes=20)        #  generate LDA model\n",
    "\n",
    "    #data_vis = pyLDAvis.gensim.prepare(data_model, data_corpus, data_dict)        # Visualise LDA Model\n",
    "    #pyLDAvis.save_html(data=data_vis,\n",
    "    #                    fileobj=out + 'Data_vis.html')\n",
    "    #data_vis\n",
    "    return data_model, data_corpus, data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users, messages = loadData(path + msg)\n",
    "grouped_messages =  getMessageGroups(messages, 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'58146'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_messages.keys()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "message_count_dict={}\n",
    "\n",
    "for k,v in grouped_messages.items():\n",
    "   message_count_dict[k] = len(v) \n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "sorted_message_count_dict = sorted(message_count_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_message_count_dict.reverse()\n",
    "\n",
    "sorted_message_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_user_data =[]\n",
    "\n",
    "min_messages = 3\n",
    "max_messages = 300\n",
    "\n",
    "grouped_messages_by_user = grouped_messages\n",
    "for k in grouped_messages.keys():\n",
    "    v = grouped_messages[k]\n",
    "    if (len(v)<max_messages and len(v)>min_messages):\n",
    "        grouped_user_data.append(messages_vectorizer(v))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_topic_model = model(grouped_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use\n",
      "plant\n",
      "soil\n",
      "control\n",
      "water\n",
      "crop\n",
      "feed\n",
      "diseas\n",
      "fertil\n",
      "cow\n",
      "maiz\n",
      "best\n",
      "farm\n",
      "anim\n",
      "manur\n",
      "make\n",
      "caus\n",
      "weed\n",
      "spray\n",
      "appli\n",
      "well\n",
      "good\n",
      "product\n",
      "dri\n",
      "seed\n"
     ]
    }
   ],
   "source": [
    "the_model = users_topic_model[0]\n",
    "the_corpus = users_topic_model[1]\n",
    "word_dict = users_topic_model[2]\n",
    "\n",
    "the_model.get_document_topics(the_corpus[0])\n",
    "\n",
    "joblib.dump(users_topic_model, '../Outputs/users_topic_model.pkl' ) \n",
    "\n",
    "\n",
    "\n",
    "for k,v in the_model.get_topic_terms(0,topn=25):\n",
    "    print word_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyaaalbakour/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_msg_en['Clean'] = df_msg_en.apply(cleaner,axis=1)\n",
    "\n",
    "df_msg_en_q = df_msg_en[(df_msg_en['type'] == 'Q')]\n",
    "df_msg_en_a = df_msg_en[(df_msg_en['type'] == 'A')]\n",
    "\n",
    "li_msg_en_q = df_msg_en_q['Clean'].tolist()\n",
    "li_msg_en_a = df_msg_en_a['Clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = li_msg_en_q[:sample_size]\n",
    "data_model, data_corpus = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_model.get_document_topics(data_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
