{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "#import pyLDAvis.gensim\n",
    "import warnings\n",
    "import csv\n",
    "from datetime import datetime as dt\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*use @default decorator instead.*')\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))       # Changing the cell widths\n",
    "\n",
    "pd.options.display.max_rows = 30                                            # Setting the max number of rows\n",
    "pd.options.display.max_columns = 50                                         # Setting the max number of columns\n",
    "\n",
    "#pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'data/'    # Data Directory \n",
    "out = 'Outputs/'  # Output Directory\n",
    "msg = 'messages.csv'                       # Input Dataset\n",
    "\n",
    "sample_size = 5000\n",
    "topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_msg_in = pd.read_csv(path + msg)\n",
    "df_msg_en = df_msg_in[(df_msg_in['language'] == 'EN')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Message(object):\n",
    "    def __init__(self, thread_id, date_time, message_id, user_id, language, msg_type, msg_body):\n",
    "        self.thread_id = thread_id\n",
    "        self.date_time = date_time\n",
    "        self.message_id = message_id\n",
    "        self.user_id = user_id\n",
    "        self.language = language\n",
    "        self.msg_type = msg_type\n",
    "        self.msg_body = msg_body\n",
    "        \n",
    "    \n",
    "class User(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def loadData(f):\n",
    "    users = {}\n",
    "    messages = []\n",
    "    with open(f, \"r\") as data:\n",
    "        # \"thread_id\",\"date_time\",\"message_id\",\"user_id\",\"language\",\"type\",\"body\"\n",
    "        # datetime format: 2015-02-09 14:27:05\n",
    "        reader = csv.DictReader(data)\n",
    "        for row in reader:\n",
    "            user_id = row[\"user_id\"]\n",
    "            message = Message(row[\"thread_id\"],\n",
    "                              dt.strptime(row[\"date_time\"], \"%Y-%m-%d %H:%M:%S\"),\n",
    "                              row[\"message_id\"],\n",
    "                              row[\"user_id\"],\n",
    "                              row[\"language\"],\n",
    "                              row[\"type\"],\n",
    "                              row[\"body\"])\n",
    "            if user_id not in users:\n",
    "                users[user_id] = []\n",
    "            users[user_id].append(message)\n",
    "            messages.append(message)\n",
    "    return users, messages\n",
    "\n",
    "def getMessageGroups(messages, grouper):\n",
    "    groupedMessages = {}\n",
    "    for message in messages:\n",
    "        messageGroup = getattr(message, grouper)\n",
    "        if messageGroup not in groupedMessages:\n",
    "            groupedMessages[messageGroup] = []\n",
    "        groupedMessages[messageGroup].append(message)\n",
    "    return groupedMessages\n",
    "        \n",
    "\n",
    "def cleaner(row):\n",
    "    '''Function to clean the text data and prep for further analysis'''\n",
    "    stops = set(stopwords.words(\"english\"))     # Creating a set of Stopwords\n",
    "    p_stemmer = PorterStemmer()                 # Creating the stemmer model\n",
    "    text = row['body'].lower()                   # Converts to lower case\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)          # Removes punctuation\n",
    "    text = re.sub(\"cyclist\",\"cycl\",text)         # Manual intervention for 'cyclist'\n",
    "    text = text.split()                          # Splits the data into individual words \n",
    "    text = [w for w in text if not w in stops]   # Removes stopwords\n",
    "    text = [p_stemmer.stem(i) for i in text]     # Stemming (reducing words to their root)\n",
    "    return text\n",
    "\n",
    "\n",
    "def messages_vectorizer(messages):\n",
    "    '''Function to take a message object and convert it to a list of terms'''\n",
    "    stops = set(stopwords.words(\"english\"))     # Creating a set of Stopwords\n",
    "    p_stemmer = PorterStemmer()                 # Creating the stemmer model\n",
    "    text = ''\n",
    "    for m in messages:\n",
    "        text = text + ' ' + m.msg_body.lower()          # Converts to lower case\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    text = text.split()                          # Splits the data into individual words \n",
    "    text = [w for w in text if not w in stops]   # Removes stopwords\n",
    "    text = [p_stemmer.stem(i) for i in text]     # Stemming (reducing words to their root)\n",
    "    return text\n",
    "\n",
    "def model(data):\n",
    "    data_dict = corpora.Dictionary(data)                       # Creates an id <-> term dictionary\n",
    "    data_corpus = [data_dict.doc2bow(text) for text in data]     # convert tokenized documents into a document-term matrix\n",
    "    data_model = gensim.models.ldamodel.LdaModel(data_corpus, \n",
    "                                                   num_topics=topics, \n",
    "                                                   id2word = data_dict,\n",
    "                                                   passes=20)        #  generate LDA model\n",
    "\n",
    "    #data_vis = pyLDAvis.gensim.prepare(data_model, data_corpus, data_dict)        # Visualise LDA Model\n",
    "    #pyLDAvis.save_html(data=data_vis,\n",
    "    #                    fileobj=out + 'Data_vis.html')\n",
    "    #data_vis\n",
    "    return data_model, data_corpus, data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users, messages = loadData(path + msg)\n",
    "grouped_messages =  getMessageGroups(messages, 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'58146'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_messages.keys()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "message_count_dict={}\n",
    "\n",
    "for k,v in grouped_messages.items():\n",
    "   message_count_dict[k] = len(v) \n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "sorted_message_count_dict = sorted(message_count_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_message_count_dict.reverse()\n",
    "\n",
    "sorted_message_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_user_data =[]\n",
    "\n",
    "min_messages = 3\n",
    "max_messages = 300\n",
    "\n",
    "grouped_messages_by_user = grouped_messages\n",
    "for k in grouped_messages.keys():\n",
    "    v = grouped_messages[k]\n",
    "    if (len(v)<max_messages and len(v)>min_messages):\n",
    "        grouped_user_data.append(messages_vectorizer(v))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_user_ids=[]\n",
    "for k in grouped_messages.keys():\n",
    "    v = grouped_messages[k]\n",
    "    if (len(v)<max_messages and len(v)>min_messages):\n",
    "        grouped_user_ids.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15716"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_topic_model = model(grouped_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use\n",
      "plant\n",
      "soil\n",
      "control\n",
      "water\n",
      "crop\n",
      "feed\n",
      "diseas\n",
      "fertil\n",
      "cow\n",
      "maiz\n",
      "best\n",
      "farm\n",
      "anim\n",
      "manur\n",
      "make\n",
      "caus\n",
      "weed\n",
      "spray\n",
      "appli\n",
      "well\n",
      "good\n",
      "product\n",
      "dri\n",
      "seed\n"
     ]
    }
   ],
   "source": [
    "the_model = users_topic_model[0]\n",
    "the_corpus = users_topic_model[1]\n",
    "word_dict = users_topic_model[2]\n",
    "\n",
    "the_model.get_document_topics(the_corpus[0])\n",
    "\n",
    "joblib.dump(users_topic_model, '../Outputs/users_topic_model.pkl' ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "users_topic_model = joblib.load('../Outputs/users_topic_model.pkl')\n",
    "the_model = users_topic_model[0]\n",
    "the_corpus = users_topic_model[1]\n",
    "word_dict = users_topic_model[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57, 0.025676843918688675),\n",
       " (107, 0.015399708681109186),\n",
       " (90, 0.014746814499382579),\n",
       " (443, 0.010094611292244492),\n",
       " (528, 0.008931766023864798),\n",
       " (29, 0.0089088351381121419),\n",
       " (485, 0.0083548929822330285),\n",
       " (768, 0.0083103290436338881),\n",
       " (96, 0.0080724065998098241),\n",
       " (45, 0.0077221438541055188),\n",
       " (8, 0.0073565709265647128),\n",
       " (203, 0.0069324333543020087),\n",
       " (109, 0.0063034868075286729),\n",
       " (13, 0.0060325985617136653),\n",
       " (201, 0.0059108217737852166),\n",
       " (625, 0.0054885687897984264),\n",
       " (121, 0.005394125290598518),\n",
       " (468, 0.0052616319549721978),\n",
       " (629, 0.0051970202486548841),\n",
       " (53, 0.0049493874865910829)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.get_topic_terms(0,topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Outputs/topics.csv', 'wb') as csvfile:\n",
    "    \n",
    "    mywriter = csv.writer(csvfile, delimiter=',',quotechar='\"')\n",
    "    \n",
    "    for t in range(10):\n",
    "        topic_words= ['T'+str(t)]\n",
    "    \n",
    "        for k,v in the_model.get_topic_terms(t,topn=25):\n",
    "            word = word_dict[k]\n",
    "            topic_words.append(word)\n",
    "        \n",
    "        mywriter.writerow(topic_words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "topics_to_users={}\n",
    "for j in range(10):\n",
    "    topics_to_users[j]={}\n",
    "\n",
    "\n",
    "\n",
    "with open('../Outputs/users.csv', 'wb') as csvfile:    \n",
    "    mywriter = csv.writer(csvfile, delimiter=',',quotechar='\"')\n",
    "    \n",
    "    for i in range(len(grouped_user_ids)):\n",
    "        user_id= grouped_user_ids[i]\n",
    "        scores= the_model.get_document_topics(the_corpus[i])\n",
    "        scores_dict={}\n",
    "        for score in scores:\n",
    "            scores_dict[score[0]]=score[1]\n",
    "        scores_arr=[]\n",
    "        for j in range(10):\n",
    "            if j in scores_dict.keys():\n",
    "                scores_arr.append(scores_dict[j])\n",
    "                topics_to_users[j][user_id]=scores_dict[j]                \n",
    "            else:\n",
    "                scores_arr.append(0.0)\n",
    "                topics_to_users[j][user_id]=0.0\n",
    "        output_arr=[]\n",
    "        output_arr.append(user_id)\n",
    "        for s in scores_arr:\n",
    "            output_arr.append(str(s))\n",
    "            \n",
    "        mywriter.writerow(output_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def top_users_for_topic(topics_to_users, t):\n",
    "    scores = topics_to_users[t]\n",
    "    sorted_scores= sorted(scores.items(), key=operator.itemgetter(1))\n",
    "    sorted_scores.reverse()\n",
    "    return sorted_scores[:50]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1459', 0.99189112516741851),\n",
       " ('49673', 0.97499810173704893),\n",
       " ('14717', 0.97428360760572064),\n",
       " ('23385', 0.97428166864439669),\n",
       " ('17860', 0.97272416999057731),\n",
       " ('13865', 0.97096476072844218),\n",
       " ('51676', 0.96999552501185571),\n",
       " ('13704', 0.96538106739704577),\n",
       " ('2644', 0.96249484240295391),\n",
       " ('54040', 0.9608654320192096),\n",
       " ('46133', 0.95908736500893133),\n",
       " ('46023', 0.95908394590218971),\n",
       " ('18873', 0.95713930875534525),\n",
       " ('58525', 0.95713911928013873),\n",
       " ('28227', 0.95713660317641702),\n",
       " ('41826', 0.95713316574522311),\n",
       " ('5328', 0.95499611046812993),\n",
       " ('35869', 0.95499598780632411),\n",
       " ('38083', 0.95499558338710899),\n",
       " ('10089', 0.9549954751254951),\n",
       " ('40222', 0.95499473111816346),\n",
       " ('40072', 0.95499459461587455),\n",
       " ('33500', 0.95499406274016452),\n",
       " ('39527', 0.95499281705296413),\n",
       " ('56284', 0.95056133264867304),\n",
       " ('14106', 0.9499906597505634),\n",
       " ('473', 0.94705227191529129),\n",
       " ('44037', 0.94705110016298688),\n",
       " ('787', 0.9459629490115069),\n",
       " ('14864', 0.94524205576659115),\n",
       " ('53256', 0.94374585968793612),\n",
       " ('11798', 0.93999656427544021),\n",
       " ('35599', 0.9399956289556467),\n",
       " ('40140', 0.9399941591791533),\n",
       " ('52784', 0.93999337519799131),\n",
       " ('41706', 0.93870601248672514),\n",
       " ('44814', 0.93570775301009623),\n",
       " ('51620', 0.93570757313315056),\n",
       " ('7590', 0.93570609988167541),\n",
       " ('55507', 0.93205114823500079),\n",
       " ('6734', 0.93075874682863624),\n",
       " ('13649', 0.93075624122635314),\n",
       " ('50877', 0.93074555698665573),\n",
       " ('42268', 0.92958808363992351),\n",
       " ('3206', 0.92901053748394291),\n",
       " ('25753', 0.92750378315381332),\n",
       " ('45395', 0.92548364077759604),\n",
       " ('22457', 0.92499574148175645),\n",
       " ('7880', 0.92499485115818481),\n",
       " ('52677', 0.92499483972198804)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_users_for_topic (topics_to_users,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.18597967244251989),\n",
       " (3, 0.44721879252540586),\n",
       " (5, 0.18151345139411518),\n",
       " (7, 0.096410220768994012),\n",
       " (9, 0.078102739582906852)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.get_document_topics(the_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyaaalbakour/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_msg_en['Clean'] = df_msg_en.apply(cleaner,axis=1)\n",
    "\n",
    "df_msg_en_q = df_msg_en[(df_msg_en['type'] == 'Q')]\n",
    "df_msg_en_a = df_msg_en[(df_msg_en['type'] == 'A')]\n",
    "\n",
    "li_msg_en_q = df_msg_en_q['Clean'].tolist()\n",
    "li_msg_en_a = df_msg_en_a['Clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = li_msg_en_q[:sample_size]\n",
    "data_model, data_corpus = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_model.get_document_topics(data_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
